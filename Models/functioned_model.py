# -*- coding: utf-8 -*-
"""Functioned_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_PMEF39tTr6DHgR0uBFVbGst_EDSkeeo
"""

# Library 'datasets' needs to be installed separately while using Google Colab
#!pip install datasets

# Importing all required libraries
import sys
import gc
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datasets import Dataset
from tqdm.auto import tqdm
from transformers import PreTrainedTokenizerFast
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

# Importing data
with open('final_data.json') as f:
    data = json.load(f)

# Converting the data into a dataframe and assigning a column name for the text data
df1 = pd.DataFrame(data)
df1 = df1.rename(columns={'0': "file"})

# Assigning labels to the data, the first 100 files are AI generated, and the next 100 are human written.
df1 = df1.assign(label=1)
for i in range(400,800):
    df1.label[i] = 0
df = df1.reindex()
df.reset_index(drop=True, inplace=True)

# Setting configuration parameters
LOWERCASE = False
VOCAB_SIZE = 30522

# Creating Tokenizer, Normalizing and Pre-Tokenizing
raw_tokenizer = Tokenizer(models.BPE(unk_token="[UNK]"))
raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])
raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()

# Configuring BPE Trainer
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)

# Splitting the dataset into training and testing sets
X = df.drop(columns=['label'])
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Training the tokenizer
dataset = Dataset.from_pandas(X_test[['file']])
def train_corp_iter():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["file"]
raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)

# Creating PreTrained Tokenizer object using the trained raw tokenizer
tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=raw_tokenizer,
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)

# Tokenizing Testing and Training data
tokenized_texts_test = []
for text in tqdm(X_test['file'].tolist()):
    tokenized_texts_test.append(tokenizer.tokenize(text))

tokenized_texts_train = []
for text in tqdm(X_train['file'].tolist()):
    tokenized_texts_train.append(tokenizer.tokenize(text))

# TF-IDF Vectorizer Configuration and Fitting on Test Data
def dummy(text):
    return text
vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',
    tokenizer = dummy,
    preprocessor = dummy,
    token_pattern = None, strip_accents='unicode')
vectorizer.fit(tokenized_texts_test)

# Getting vocab
vocab = vectorizer.vocabulary_

# Reconfiguring Vectorizer and transforming training dataset, and testing dataset
vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,
                            analyzer = 'word',
                            tokenizer = dummy,
                            preprocessor = dummy,
                            token_pattern = None, strip_accents='unicode'
                            )

tf_train = vectorizer.fit_transform(tokenized_texts_train)
tf_test = vectorizer.transform(tokenized_texts_test)

# Model configuration and training
clf = MultinomialNB(alpha=0.005)
sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss="modified_huber")
weights = [0.57, 0.43]
ensemble = VotingClassifier(estimators=[('mnb', clf),
                                          ('sgd', sgd_model)],
                              weights=weights, voting='soft', n_jobs=-1)
ensemble.fit(tf_train, y_train)
gc.collect()

# Getting and formatiing predictions on test data
final_preds = ensemble.predict_proba(tf_test)[:, 1]
formatted_values = [f"{val:f}" for val in final_preds]
# Plottin the roc curve based of the probabilities
def plot_roc_curve(true_y, y_prob):
    fpr, tpr, thresholds = roc_curve(true_y, y_prob)
    plt.plot(fpr, tpr)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
plot_roc_curve(y_test.to_numpy(), np.array(formatted_values).astype(float))
print(f'model 1 AUC score: {roc_auc_score(y_test.to_numpy(), np.array(formatted_values).astype(float))}')
floored_values = []
for val in formatted_values:
  if float(val)<0.65:
    floored_values.append(0)
  else:
    floored_values.append(1)

# Compute precision, recall, and F1 score for class 0
test_labels = y_test.tolist()
cm = confusion_matrix(test_labels, floored_values)
precision_0 = precision_score(test_labels, floored_values, pos_label=0)
recall_0 = recall_score(test_labels, floored_values, pos_label=0)
f1_0 = f1_score(test_labels, floored_values, pos_label=0)

# Compute precision, recall, and F1 score for class 1
precision_1 = precision_score(test_labels, floored_values, pos_label=1)
recall_1 = recall_score(test_labels, floored_values, pos_label=1)
f1_1 = f1_score(test_labels, floored_values, pos_label=1)

print("Confusion Matrix:")
print(cm)
print("\nClass 0:")
print("Precision:", precision_0)
print("Recall:", recall_0)
print("F1 Score:", f1_0)
print("\nClass 1:")
print("Precision:", precision_1)
print("Recall:", recall_1)
print("F1 Score:", f1_1)

def classification(value):
  k = tokenizer.tokenize(value)
  t=[]
  t.append(k)
  t_test = vectorizer.transform(t)
  fin = ensemble.predict_proba(t_test)[:, 1]
  if float(fin)<0.65:
    val = 0
  else:
    val = 1
  return [fin[0],val]

# replace entry with the string value of the pdf text
entry = df.file[533]
result = classification(entry)

print(result)